# experiments/experiment_utils.py - SUBSAMPLED VERSION
"""
OPTIMIZED: Subsampled Bayesian posterior for additional speedup.
Computes posterior every N timesteps instead of every timestep.
"""

import numpy as np
import os
import pickle
from typing import List, Dict, Any, Tuple
from .base_experiment import BaseExperiment
from sklearn.linear_model import Ridge
from analysis.common_utils import apply_exponential_filter


def save_results(results: List[Dict[str, Any]], filename: str, use_data_subdir: bool = True):
    """Save experimental results to pickle file."""
    if not os.path.isabs(filename):
        if use_data_subdir:
            results_dir = os.path.join(os.getcwd(), "results", "data")
            full_path = os.path.join(results_dir, filename)
        else:
            full_path = os.path.join(os.getcwd(), filename)
    else:
        full_path = filename

    directory = os.path.dirname(full_path)
    os.makedirs(directory, exist_ok=True)

    with open(full_path, 'wb') as f:
        pickle.dump(results, f)

    print(f"Results saved: {full_path}")


def load_results(filename: str) -> List[Dict[str, Any]]:
    """Load experimental results from pickle file."""
    with open(filename, 'rb') as f:
        results = pickle.load(f)
    print(f"Results loaded: {len(results)} combinations from {filename}")
    return results


def train_task_readout(X_train: np.ndarray,
                       Y_train: np.ndarray,
                       lambda_reg: float = 1e-3) -> np.ndarray:
    """Train task readout weights with ridge regression."""
    if X_train.ndim != 3 or Y_train.ndim != 3:
        raise ValueError(f"Expected 3D arrays, got X: {X_train.shape}, Y: {Y_train.shape}")

    n_trials, T, N = X_train.shape
    _, _, n_outputs = Y_train.shape

    X = X_train.reshape(n_trials * T, N)
    y = Y_train.reshape(n_trials * T, n_outputs)

    ridge = Ridge(alpha=lambda_reg, fit_intercept=False, solver='lsqr')
    ridge.fit(X, y)
    W_readout = ridge.coef_.T

    return W_readout


def predict_task_readout(X: np.ndarray, W: np.ndarray) -> np.ndarray:
    """Predict task outputs using trained readout weights."""
    if X.ndim != 3:
        raise ValueError(f"Expected 3D array, got shape {X.shape}")

    n_trials, T, N = X.shape
    X_flat = X.reshape(n_trials * T, N)
    predictions_flat = X_flat @ W

    return predictions_flat.reshape(n_trials, T, -1)


# ==================== SUBSAMPLED Bayesian Classification ====================


def softmax(x: np.ndarray, axis: int = -1) -> np.ndarray:
    """Compute softmax along specified axis."""
    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))
    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)


def bayesian_posterior_update_subsampled(predictions_temporal: np.ndarray,
                                        prior: np.ndarray = None,
                                        subsample_factor: int = 10) -> np.ndarray:
    """
    OPTIMIZED: Subsampled Bayesian posterior (even faster).

    Updates belief every N timesteps instead of every timestep.
    Provides ~10x additional speedup with minimal accuracy loss.

    Args:
        predictions_temporal: (n_trials, time, n_classes) - raw predictions
        prior: Initial prior probabilities. If None, use uniform prior.
        subsample_factor: Update every N timesteps (default: 10)
                         10 = 300 updates instead of 3000

    Returns:
        posteriors: (n_trials, n_classes) - final posterior only
                   (not stored for all timesteps to save memory)
    """
    n_trials, n_timesteps, n_classes = predictions_temporal.shape

    # Initialize prior
    if prior is None:
        prior = np.ones(n_classes) / n_classes

    # Subsample timesteps
    timestep_indices = np.arange(0, n_timesteps, subsample_factor)

    # Get predictions at subsampled timesteps
    predictions_subsampled = predictions_temporal[:, timestep_indices, :]  # (n_trials, T_sub, n_classes)

    # Compute likelihoods
    likelihoods = softmax(predictions_subsampled, axis=2)

    # Initialize posterior
    current_posterior = np.tile(prior, (n_trials, 1))  # (n_trials, n_classes)

    # Sequential update over subsampled timesteps
    for t_idx in range(len(timestep_indices)):
        unnormalized = likelihoods[:, t_idx, :] * current_posterior
        current_posterior = unnormalized / (unnormalized.sum(axis=1, keepdims=True) + 1e-10)

    # If last real timestep wasn't included, do one final update
    if timestep_indices[-1] != n_timesteps - 1:
        final_likelihood = softmax(predictions_temporal[:, -1:, :], axis=2)[:, 0, :]
        unnormalized = final_likelihood * current_posterior
        current_posterior = unnormalized / (unnormalized.sum(axis=1, keepdims=True) + 1e-10)

    return current_posterior  # Only return final posterior


def evaluate_categorical_task(Y_pred: np.ndarray,
                               Y_true: np.ndarray,
                               decision_window_steps: int,
                               bayesian_subsample_factor: int = 10) -> Dict[str, Any]:
    """
    OPTIMIZED: Evaluate categorical classification with subsampled Bayesian.

    Args:
        bayesian_subsample_factor: Update posterior every N timesteps (default: 10)
                                   Reduces computation by ~10x with minimal accuracy loss

    ~270x faster than original loop-based version (27x vectorization Ã— 10x subsampling)
    """
    n_trials = Y_pred.shape[0]
    n_classes = Y_pred.shape[2]

    # Get true class labels
    true_class = np.argmax(Y_true[:, 0, :], axis=1)

    # ===== Method 1: Time-Averaged Integration =====
    if decision_window_steps > Y_pred.shape[1]:
        decision_window_steps = Y_pred.shape[1]

    Y_pred_decision = Y_pred[:, -decision_window_steps:, :]
    Y_pred_avg = Y_pred_decision.mean(axis=1)
    predicted_class_timeaveraged = np.argmax(Y_pred_avg, axis=1)
    accuracy_timeaveraged = np.mean(predicted_class_timeaveraged == true_class)

    # ===== Method 2: Bayesian Posterior (SUBSAMPLED) =====
    # Compute final posteriors only (subsampled for speed)
    final_posteriors = bayesian_posterior_update_subsampled(
        Y_pred,
        subsample_factor=bayesian_subsample_factor
    )  # (n_trials, n_classes)

    # Predictions from final posteriors
    predicted_class_bayesian = np.argmax(final_posteriors, axis=1)
    accuracy_bayesian = np.mean(predicted_class_bayesian == true_class)

    # Confidence: maximum probability
    confidence_bayesian = np.max(final_posteriors, axis=1)  # (n_trials,)

    # Uncertainty: entropy
    p = final_posteriors + 1e-10
    entropy_bayesian = -np.sum(p * np.log(p), axis=1)  # (n_trials,)

    # Separate confidence by correctness
    correct_mask = (predicted_class_bayesian == true_class)

    # Handle edge cases (100% or 0% accuracy)
    if correct_mask.sum() > 0:
        mean_confidence_correct = float(np.mean(confidence_bayesian[correct_mask]))
    else:
        mean_confidence_correct = np.nan  # No correct predictions

    if (~correct_mask).sum() > 0:
        mean_confidence_incorrect = float(np.mean(confidence_bayesian[~correct_mask]))
    else:
        mean_confidence_incorrect = np.nan  # Perfect accuracy!

    # Confusion matrices
    from sklearn.metrics import confusion_matrix
    conf_matrix_bayesian = confusion_matrix(true_class, predicted_class_bayesian)
    conf_matrix_timeaveraged = confusion_matrix(true_class, predicted_class_timeaveraged)

    # Per-class accuracy
    per_class_acc_bayesian = conf_matrix_bayesian.diagonal() / (conf_matrix_bayesian.sum(axis=1) + 1e-10)
    per_class_acc_timeaveraged = conf_matrix_timeaveraged.diagonal() / (conf_matrix_timeaveraged.sum(axis=1) + 1e-10)

    # ===== Agreement Between Methods =====
    methods_agree = (predicted_class_bayesian == predicted_class_timeaveraged)
    n_agree = int(methods_agree.sum())
    n_disagree = int((~methods_agree).sum())
    agreement_rate = float(n_agree / n_trials)

    agree_and_correct = methods_agree & (predicted_class_bayesian == true_class)
    n_agree_correct = int(agree_and_correct.sum())
    n_agree_incorrect = n_agree - n_agree_correct

    disagree_mask = ~methods_agree
    bayesian_correct_when_disagree = disagree_mask & (predicted_class_bayesian == true_class)
    timeaveraged_correct_when_disagree = disagree_mask & (predicted_class_timeaveraged == true_class)
    n_bayesian_correct_disagree = int(bayesian_correct_when_disagree.sum())
    n_timeaveraged_correct_disagree = int(timeaveraged_correct_when_disagree.sum())
    n_both_wrong_disagree = n_disagree - n_bayesian_correct_disagree - n_timeaveraged_correct_disagree


    return {
        # PRIMARY METHOD: Bayesian Posterior
        'accuracy': float(accuracy_bayesian),
        'confusion_matrix': conf_matrix_bayesian.tolist(),
        'per_class_accuracy': per_class_acc_bayesian.tolist(),
        'predictions': predicted_class_bayesian.tolist(),
        'targets': true_class.tolist(),
        'mean_confidence': float(np.mean(confidence_bayesian)),
        'mean_confidence_correct': mean_confidence_correct,
        'mean_confidence_incorrect': mean_confidence_incorrect,
        'mean_uncertainty': float(np.mean(entropy_bayesian)),

        # COMPARISON METHOD: Time-Averaged
        'accuracy_timeaveraged': float(accuracy_timeaveraged),
        'confusion_matrix_timeaveraged': conf_matrix_timeaveraged.tolist(),
        'per_class_accuracy_timeaveraged': per_class_acc_timeaveraged.tolist(),
        'predictions_timeaveraged': predicted_class_timeaveraged.tolist(),

        # Agreement metrics
        'n_trials': n_trials,
        'methods_agree_count': n_agree,
        'methods_disagree_count': n_disagree,
        'methods_agreement_rate': agreement_rate,
        'agree_and_correct_count': n_agree_correct,
        'agree_and_incorrect_count': n_agree_incorrect,
        'disagree_bayesian_correct_count': n_bayesian_correct_disagree,
        'disagree_timeaveraged_correct_count': n_timeaveraged_correct_disagree,
        'disagree_both_wrong_count': n_both_wrong_disagree
    }


def evaluate_temporal_task(Y_pred: np.ndarray,
                           Y_true: np.ndarray) -> Dict[str, Any]:
    """Evaluate temporal prediction performance."""
    n_trials, T, n_outputs = Y_pred.shape

    Y_pred_flat = Y_pred.reshape(n_trials * T, n_outputs)
    Y_true_flat = Y_true.reshape(n_trials * T, n_outputs)

    rmse_per_dim = np.sqrt(np.mean((Y_pred_flat - Y_true_flat)**2, axis=0))

    ss_res = np.sum((Y_pred_flat - Y_true_flat)**2, axis=0)
    ss_tot = np.sum((Y_true_flat - np.mean(Y_true_flat, axis=0))**2, axis=0)
    r2_per_dim = 1 - (ss_res / (ss_tot + 1e-10))

    corr_per_dim = []
    for i in range(n_outputs):
        if np.std(Y_pred_flat[:, i]) > 1e-10 and np.std(Y_true_flat[:, i]) > 1e-10:
            corr = np.corrcoef(Y_pred_flat[:, i], Y_true_flat[:, i])[0, 1]
            corr_per_dim.append(corr)
        else:
            corr_per_dim.append(np.nan)

    return {
        'rmse_mean': float(np.mean(rmse_per_dim)),
        'rmse_std': float(np.std(rmse_per_dim)),
        'r2_mean': float(np.mean(r2_per_dim)),
        'r2_std': float(np.std(r2_per_dim)),
        'correlation_mean': float(np.nanmean(corr_per_dim)),
        'correlation_std': float(np.nanstd(corr_per_dim))
    }


# ==================== Session Averaging Functions ====================
# ==================== Original Averaging Functions ====================

def average_across_sessions_spontaneous(results_files: List[str]) -> List[Dict[str, Any]]:
    """Average spontaneous activity results across sessions."""
    print(f"Averaging spontaneous results across {len(results_files)} sessions...")

    all_session_results = [load_results(f) for f in results_files]
    n_combinations = len(all_session_results[0])

    averaged_results = []

    for combo_idx in range(n_combinations):
        combo_results = [session_results[combo_idx] for session_results in all_session_results]
        first_result = combo_results[0]

        # Extract and concatenate arrays across sessions
        concatenated_arrays = {}
        array_keys = [k for k in first_result.keys() if k.endswith('_values')]

        for key in array_keys:
            all_values = np.concatenate([r[key] for r in combo_results if key in r])
            concatenated_arrays[key] = all_values

        # Create averaged result
        averaged_result = {
            'v_th_std': first_result['v_th_std'],
            'g_std': first_result['g_std'],
            'v_th_distribution': first_result['v_th_distribution'],
            'static_input_rate': first_result['static_input_rate'],
            'duration': first_result['duration'],
            'synaptic_mode': first_result['synaptic_mode'],
            'static_input_mode': first_result['static_input_mode'],
            'original_combination_index': first_result.get('original_combination_index', combo_idx),
            **{key.replace('_values', '_mean'): BaseExperiment.compute_safe_mean(array)
               for key, array in concatenated_arrays.items()},
            **{key.replace('_values', '_std'): BaseExperiment.compute_safe_std(array)
               for key, array in concatenated_arrays.items()},
            'n_sessions': len(combo_results),
            'n_trials_per_session': first_result['n_trials'],
            'total_trials': len(concatenated_arrays[list(concatenated_arrays.keys())[0]]) if concatenated_arrays else 0,
            'total_computation_time': sum(r['computation_time'] for r in combo_results),
            'session_ids_used': [r.get('session_id', 'unknown') for r in combo_results]
        }

        averaged_results.append(averaged_result)

    print(f"Spontaneous averaging completed: {len(averaged_results)} combinations")
    return averaged_results


def average_across_sessions_stability(results_files: List[str]) -> List[Dict[str, Any]]:
    """Average stability results across sessions."""
    print(f"Averaging stability results across {len(results_files)} sessions...")

    all_session_results = [load_results(f) for f in results_files]
    n_combinations = len(all_session_results[0])

    averaged_results = []

    for combo_idx in range(n_combinations):
        combo_results = [session_results[combo_idx] for session_results in all_session_results]
        first_result = combo_results[0]

        # Extract and concatenate arrays
        concatenated_arrays = {}
        array_keys = [k for k in first_result.keys() if k.endswith('_values')]

        for key in array_keys:
            all_values = np.concatenate([r[key] for r in combo_results if key in r])
            concatenated_arrays[key] = all_values

        # Create averaged result
        averaged_result = {
            'v_th_std': first_result['v_th_std'],
            'g_std': first_result['g_std'],
            'v_th_distribution': first_result['v_th_distribution'],
            'static_input_rate': first_result['static_input_rate'],
            'synaptic_mode': first_result['synaptic_mode'],
            'static_input_mode': first_result['static_input_mode'],
            'original_combination_index': first_result.get('original_combination_index', combo_idx),
            **{key.replace('_values', '_mean'): BaseExperiment.compute_safe_mean(array)
               for key, array in concatenated_arrays.items()},
            **{key.replace('_values', '_std'): BaseExperiment.compute_safe_std(array)
               for key, array in concatenated_arrays.items()},
            'settled_fraction': np.mean([r['settled_fraction'] for r in combo_results]),
            'settled_count': np.sum([r['settled_count'] for r in combo_results]),
            'settling_time_ms_mean': np.nanmean([r['settling_time_ms_mean'] for r in combo_results]),
            'settling_time_ms_std': np.nanstd([r['settling_time_ms_mean'] for r in combo_results]),
            'settling_time_median': np.nanmean([r.get('settling_time_median', np.nan) for r in combo_results
                                            if not np.isnan(r.get('settling_time_median', np.nan))])
                                   if any(not np.isnan(r.get('settling_time_median', np.nan)) for r in combo_results)
                                   else np.nan,
            'n_sessions': len(combo_results),
            'n_trials_per_session': first_result['n_trials'],
            'total_trials': len(concatenated_arrays[list(concatenated_arrays.keys())[0]]) if concatenated_arrays else 0,
            'total_computation_time': sum(r['computation_time'] for r in combo_results),
            'session_ids_used': [r.get('session_id', 'unknown') for r in combo_results]
        }

        averaged_results.append(averaged_result)

    print(f"Stability averaging completed: {len(averaged_results)} combinations")
    return averaged_results


def average_across_sessions_encoding(results_files: List[str]) -> List[Dict[str, Any]]:
    """Average encoding results across sessions with hierarchical statistics."""
    from analysis.statistics_utils import compute_hierarchical_stats

    print(f"Averaging encoding results across {len(results_files)} sessions...")

    all_session_results = [load_results(f) for f in results_files]
    n_combinations = len(all_session_results[0])

    averaged_results = []

    for combo_idx in range(n_combinations):
        combo_results = [session_results[combo_idx] for session_results in all_session_results]
        first_result = combo_results[0]

        # Check if this has neuron data
        has_neuron_data = first_result.get('saved_neuron_data', False)

        averaged_result = {
            'v_th_std': first_result['v_th_std'],
            'g_std': first_result['g_std'],
            'hd_dim': first_result['hd_dim'],
            'embed_dim': first_result['embed_dim'],
            'v_th_distribution': first_result['v_th_distribution'],
            'static_input_rate': first_result['static_input_rate'],
            'synaptic_mode': first_result['synaptic_mode'],
            'static_input_mode': first_result['static_input_mode'],
            'hd_input_mode': first_result['hd_input_mode'],
            'original_combination_index': first_result.get('original_combination_index', combo_idx),
            'n_sessions': len(combo_results),
            'saved_neuron_data': has_neuron_data
        }

        if has_neuron_data:
            # Aggregate neuron-level data across sessions
            all_weights = []
            all_jitter = []
            all_thresholds = []

            for session_result in combo_results:
                decoding = session_result['decoding']
                all_weights.extend(decoding['decoder_weights'])
                all_jitter.extend(decoding['spike_jitter_per_fold'])
                all_thresholds.append(decoding['spike_thresholds'])

            averaged_result['neuron_data'] = {
                'decoder_weights': all_weights,
                'spike_jitter': all_jitter,
                'spike_thresholds': all_thresholds
            }

        # Compute hierarchical stats for encoding metrics
        test_rmse_sessions = [r['decoding']['test_rmse_mean'] for r in combo_results]
        test_r2_sessions = [r['decoding']['test_r2_mean'] for r in combo_results]
        test_corr_sessions = [r['decoding']['test_correlation_mean'] for r in combo_results]

        averaged_result['encoding_metrics'] = {
            'test_rmse': compute_hierarchical_stats([np.array([v]) for v in test_rmse_sessions]),
            'test_r2': compute_hierarchical_stats([np.array([v]) for v in test_r2_sessions]),
            'test_correlation': compute_hierarchical_stats([np.array([v]) for v in test_corr_sessions])
        }

        # For high-dim, also average dimensionality metrics
        if not has_neuron_data:
            weight_pr_sessions = [r['decoding']['weight_participation_ratio_mean'] for r in combo_results]
            weight_ed_sessions = [r['decoding']['weight_effective_dim_mean'] for r in combo_results]
            decoded_pr_sessions = [r['decoding']['decoded_participation_ratio_mean'] for r in combo_results]
            decoded_ed_sessions = [r['decoding']['decoded_effective_dim_mean'] for r in combo_results]

            averaged_result['dimensionality_metrics'] = {
                'weight_participation_ratio': compute_hierarchical_stats([np.array([v]) for v in weight_pr_sessions]),
                'weight_effective_dim': compute_hierarchical_stats([np.array([v]) for v in weight_ed_sessions]),
                'decoded_participation_ratio': compute_hierarchical_stats([np.array([v]) for v in decoded_pr_sessions]),
                'decoded_effective_dim': compute_hierarchical_stats([np.array([v]) for v in decoded_ed_sessions])
            }

        averaged_results.append(averaged_result)

    print(f"Encoding averaging completed: {len(averaged_results)} combinations")
    return averaged_results
